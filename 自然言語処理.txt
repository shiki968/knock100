自然言語 (NL, Natural Language)とは、日本語や英語のような 自然発生的に生まれた言語 のことを指し、プログラミング言語のような人工言語(Artificial Language)とは対比の存在です。

自然言語処理 (NLP, Natural Language Processing)とは、人間が日常的に使っている 自然言語をコンピュータに処理させる技術 のことです。
自然言語処理を用いたタスクには、文書分類・機械翻訳・文書要約・質疑応答・対話などがあります。


自然言語処理でよく使われるワードとして、以下のようなものがあげられます。

トークン：自然言語を解析する際、文章の最小単位して扱われる文字や文字列のこと。
タイプ：単語の種類を表す用語。
文章：まとまった内容を表す文のこと。自然言語処理では一文を指すことが多い。
文書：複数の文章から成るデータ一件分を指すことが多い。
コーパス：文書または音声データにある種の情報を与えたデータ。
シソーラス：単語の上位/下位関係、部分/全体関係、同義関係、類義関係などによって単語を分類し、体系づけた類語辞典・辞書。
形態素：意味を持つ最小の単位。「食べた」という単語は、2つの形態素「食べ」と「た」に分解できる。
単語：単一または複数の形態素から構成される小さな単位。
表層：原文の記述のこと。
原形：活用する前の記述のこと。
特徴：文章や文書から抽出された情報のこと。
辞書：自然言語処理では、単語のリストを指す。


日本語は単語の区切りが難しい


1.2.1 形態素解析とNgram（単語分割）

形態素解析とは、辞書を利用して形態素に分割し、さらに形態素ごとに品詞などのタグ付け（情報の付与）を行うことを指します。

Ngram とは、 N文字ごとに単語を切り分ける 、または N単語ごとに文章を切り分ける 解析手法のことです。

1文字、あるいは1単語ごとに切り出したものを モノグラム 、
2文字（単語）ごとに切り出したものを バイグラム 、
3文字（単語）ごとに切り出したものを トリグラム と呼びます。

例えば「あいうえお」という文の文字のモノグラム・バイグラム・トリグラムを考えてみると以下のようになります。

モノグラム：{あ, い, う, え, お}
バイグラム：{あい, いう, うえ, えお}
トリグラム：{あいう, いうえ, うえお}

Ngram は形態素解析のように辞書や文法的な解説が不要であるため、 言語に関係なく 用いることができます。
また Ngram は特徴抽出の漏れが発生しにくいメリットがありますが、ノイズが大きくなるデメリットがあります。

例えば、「東京都の世界一有名なIT企業」という少し長い文字列について検索する際、バイグラムによって文字列を分割し検索すると「京都」の企業がヒットする可能性が出てしまいます。 なぜなら「東京都」の文字バイグラムを列挙すると{東京, 京都}となるからです。
形態素解析を適切に用いるとこのようなことは起こりませんが、性能の高い辞書を用意する必要があります。



1.2.2 MeCab（めかぶ）

import MeCab
mecab = MeCab.Tagger("-Owakati")	分かち書き
print(mecab.parse("明日は晴れるでしょう。"))
# 出力結果
明日 は 晴れる でしょ う 。

mecab = MeCab.Tagger("-Ochasen")	形態素解析
print(mecab.parse("明日は晴れるでしょう。"))

"""
# 出力結果
明日	アシタ	明日	名詞-副詞可能		
は	ハ	は	助詞-係助詞		
晴れる	ハレル	晴れる	動詞-自立	一段	基本形
でしょ	デショ	です	助動詞	特殊・デス	未然形
う	ウ	う	助動詞	不変化型	基本形
。	。	。	記号-句点		

EOS
"""

.parse("文章")とすることにより引数の文章を指定された形式で出力させることができます。



1.2.3 janome

パッケージのインストールが容易

from janome.tokenizer import Tokenizer
tokenizer = Tokenizer()  # Tokenizerオブジェクトの作成
tokens = tokenizer.tokenize("pythonの本を読んだ")
for token in tokens:
    print(token)
    print()

#分かち書き（文字列を , で区切る）
tokens = t.tokenize("pythonの本を読んだ", wakati=True)
print(tokens)


Tokenに対しての処理

#表層形（文中において文字列として実際に出現する形式）
tokens = t.tokenize("pythonの本を読んだ")
for token in tokens:
    print(token.surface)

# 品詞
tokens = t.tokenize("pythonの本を読んだ")
for token in tokens:
    print(token.part_of_speech)

品詞の抜き出し
part_of_speech = token.part_of_speech.split(",")[0]


1.2.5 split関数（Pythonの組み込み関数）

区切り文字で切り分けてリスト化

fruits = "banana, apple, strawberry"
print(fruits.split(","))  # list
# 出力結果
["banana", " apple", " strawberry"]


1.2.7 Ngram



